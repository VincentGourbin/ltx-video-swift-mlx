LTX-2 Video Generation
======================
Prompt: A beaver building a dam in a forest stream, detailed fur, water splashing, natural lighting
Output: docs/examples/beaver-dam/distilled-upscaler.mp4
Resolution: 768x512
Frames: 25
Model: distilled
Seed: 42
Two-stage: enabled
Prompt enhancement: enabled

Two-stage pipeline: 384x256 -> upscale 2x -> 768x512
  Base model: distilled
Creating pipeline...
Pipeline created
Loading models (this may take a while)...
[LTX] Loading models for LTX-2 Distilled (~16GB)...
  Loading Gemma model... (10%)
[LTX] Downloading Gemma text encoder for LTX-2 Distilled (~16GB) (if needed)...
  Preparing to download text encoder... (0%)
  Text encoder already downloaded (100%)
  Tokenizer already downloaded (100%)
[LTX] [TIME] Gemma download check: 0.0s
[LTX] Loading Gemma3 model from /Users/vincent/Library/Caches/models/ltx-distilled/text_encoder...
[Gemma3] Config: 48 layers, 3840 hidden, 16 heads
[Gemma3] Loaded 2189 weight tensors from safetensors
[Gemma3] Weights loaded and evaluated successfully
[LTX] [TIME] Gemma load: 7.4s — 48 layers
  Loading tokenizer... (20%)
[LTX] Loading tokenizer from /Users/vincent/Library/Caches/models/ltx-distilled/tokenizer...
[LTX] [TIME] Tokenizer load: 3.0s
  Loading LTX-2 weights... (30%)
[LTX] Downloading LTX-2 components for LTX-2 Distilled (~16GB) (if needed)...
  Downloading connector weights... (35%)
  Connector weights already downloaded (100%)
[LTX] Loading connector weights from: /Users/vincent/Library/Caches/models/ltx-distilled/connectors/diffusion_pytorch_model.safetensors
[LTX] Loaded 59 tensors
[LTX] Mapped 30 text encoder weights
  Downloading transformer weights... (40%)
  Unified weights already downloaded (100%)
[LTX] Loading transformer weights from: /Users/vincent/Library/Caches/models/ltx-distilled/ltx-2-19b-distilled.safetensors
[LTX] Loaded 4052 tensors via mmap
[LTX] Mapped 1215 transformer weights (from 1263 total)
[LTX] Extracted 1215 transformer weights in 1.5s
  Downloading VAE weights... (80%)
  VAE weights already downloaded (100%)
[LTX] Loading VAE weights from: /Users/vincent/Library/Caches/models/ltx-distilled/vae/diffusion_pytorch_model.safetensors
[LTX] Loaded 184 tensors
[LTX] Mapped 92 VAE decoder weights (encoder weights skipped)
[LTX] VAE mapped keys: ["conv_in.conv.bias", "conv_in.conv.weight", "conv_out.conv.bias", "conv_out.conv.weight", "mean_of_means", "std_of_means", "up_blocks_0.res_blocks.0.conv1.conv.bias", "up_blocks_0.res_blocks.0.conv1.conv.weight", "up_blocks_0.res_blocks.0.conv2.conv.bias", "up_blocks_0.res_blocks.0.conv2.conv.weight"]...
[LTX] [TIME] Component downloads: 1.6s
  Loading transformer... (50%)
[LTX] Applying 1215 transformer weights...
[LTX] Converted 49 float32 parameters to bfloat16
[LTX] Applied 1215 weights to transformer (0 unmatched)
[LTX] [TIME] Apply transformer weights: 0.0s
[LTX] [TIME] Eval transformer weights: 3.7s
  Loading VAE decoder... (70%)
[LTX] VAE config: timestep_conditioning=false
[LTX] Applying 92 VAE weights...
[LTX] VAE: 42 model params NOT loaded:
[LTX]   missing: last_scale_shift_table
[LTX]   missing: last_time_embedder.timestep_embedder.linear_1.bias
[LTX]   missing: last_time_embedder.timestep_embedder.linear_1.weight
[LTX]   missing: last_time_embedder.timestep_embedder.linear_2.bias
[LTX]   missing: last_time_embedder.timestep_embedder.linear_2.weight
[LTX]   missing: timestep_scale_multiplier
[LTX]   missing: up_blocks_0.res_blocks.0.scale_shift_table
[LTX]   missing: up_blocks_0.res_blocks.1.scale_shift_table
[LTX]   missing: up_blocks_0.res_blocks.2.scale_shift_table
[LTX]   missing: up_blocks_0.res_blocks.3.scale_shift_table
[LTX] Applied 92 weights to VAE (0 unmatched)
[LTX] [TIME] VAE load: 0.0s
  Loading text encoder... (90%)
[LTX] Applying 30 text encoder weights...
[LTX] Applied 30 weights to TextEncoder (0 unmatched)
[LTX] [TIME] TextEncoder load: 0.0s
  Models loaded successfully (100%)
[LTX] All models loaded successfully
Models loaded in 15.7s

Generating video...
Downloading upscaler weights (if needed)...
Upscaler weights ready
[LTX] Two-stage generation: 384x256 → 768x512
[LTX] [MEM] two-stage start: active=47316MB peak=47321MB cache=7175MB
[LTX] Enhancing prompt with Gemma...
[LTX] Enhancing prompt: "A beaver building a dam in a forest stream, detailed fur, water splashing, natural lighting"
[LTX] Stop tokens: eos=1, end_of_turn=106
[LTX] Chat template failed (missingChatTemplate), falling back to raw tokenization
[LTX] Enhancement input (fallback): 958 tokens
[LTX] Generated 221 tokens in 26.7s
[LTX] Enhanced prompt: "Style: realistic with natural lighting. A beaver, its fur a rich brown color and detailed down to individual hairs, is actively building a dam in a clear forest stream. The beaver stands on its hind legs, using its strong front paws to carry a small branch towards the growing structure of interwoven sticks and mud. Water splashes gently as the beaver pushes the branch into place. Sunlight filters through the dense canopy above, creating dappled light patterns on the water's surface and illuminating the beaver’s focused expression. The stream gurgles softly—a constant flow of water over rocks—and birds chirp intermittently from nearby trees. As the beaver secures the branch, a small amount of mud is displaced, causing more splashing sounds. It then kneels to gather additional materials, its fur dampening slightly with each splash. A quiet rustling of leaves indicates subtle movements in the surrounding foliage. The beaver pauses, head cocked, listening intently while droplets fall from its fur—their sound effect a gentle patter—before resuming work on the dam, branches and mud combining sounds of construction echoing across calm water."
[LTX] [MEM] Phase textEncoding: cache limit set to 512MB
[LTX] Tokenized: [1, 1024], padding=802, active=222
[LTX]   First 5 tokens: [0, 0, 0, 0, 0]
[LTX]   Last 10 tokens: [14791, 26614, 12054, 529, 5431, 159689, 3418, 16680, 1813, 236761]
[LTX] Running Gemma forward pass...
[LTX] Got 49 hidden states from Gemma
[LTX] [TextEnc] Gemma hidden states: 49 layers
[LTX] [TextEnc]   layer_0: dtype=bfloat16, mean=-0.012115555, std=0.96553904
[LTX] [TextEnc]   layer_1: dtype=bfloat16, mean=0.27905217, std=10.63557
[LTX] [TextEnc]   layer_24: dtype=bfloat16, mean=15.908468, std=1016.9608
[LTX] [TextEnc]   layer_47: dtype=bfloat16, mean=50.005333, std=2747.8745
[LTX] [TextEnc]   layer_48: dtype=bfloat16, mean=0.027022108, std=1.7558199
[LTX] [TextEnc] After norm_concat: dtype=bfloat16, shape=[1, 1024, 188160], mean=2.0397628e-08, std=0.014533132
[LTX] [TextEnc] After FE: dtype=bfloat16, shape=[1, 1024, 3840], mean=-1.0738355e-05, std=0.037795957
[LTX] [TextEnc] After connector: dtype=bfloat16, mean=0.0027878224, std=1.0000045
[LTX] Text encoding: [1, 1024, 3840], mean=0.0028076172
[LTX] Text mask: [1, 1024], active=1024/1024
[LTX] Text encoding: 5.2s
[LTX] Unloading Gemma model to free memory...
[LTX] Gemma model unloaded
[LTX] Using seed: 42
[LTX] [MEM] Phase denoising: cache limit set to 2048MB
[LTX] === Stage 1: Half-resolution denoising (8 steps) ===
[LTX] Stage 1 latent: 4x8x12
[LTX] Stage 1 using distilled sigmas (8 steps)
  Step 1/8 (σ=1.0000)
[LTX] Step 0: patchified [1, 384, 128], σ=1.0000
[LTX] Transformer input shapes:
[LTX]   latent: [1, 384, 128]
[LTX]   context: [1, 1024, 3840]
[LTX]   timesteps: [1]
[LTX]   latentShape: (frames: 4, height: 8, width: 12)
[LTX]   [TIME] patchifyProj: 0.001s
[LTX]   after patchifyProj: [1, 384, 4096]
[LTX]   [TIME] prepareTimestep: 0.023s
[LTX]   [TIME] prepareContext: 0.009s
[LTX]   [TIME] preparePositionalEmbeddings (RoPE): 0.202s
[LTX]   RoPE cos shape: [1, 32, 384, 64]
[LTX]   RoPE sin shape: [1, 32, 384, 64]
[LTX]   [TIME] all transformer blocks: 3.406s
[LTX]   [TIME] processOutput: 0.002s
[LTX]   [TIME] TOTAL transformer forward: 3.643s
[LTX]   Step 0: σ=1.0000→0.9937, vel mean=0.0291, std=1.0944, latent mean=0.0047, std=0.9935
[LTX]   Step 0 time: 3.64s
  Step 2/8 (σ=0.9937)
[LTX] Step 1: patchified [1, 384, 128], σ=0.9937
[LTX] Transformer input shapes:
[LTX]   latent: [1, 384, 128]
[LTX]   context: [1, 1024, 3840]
[LTX]   timesteps: [1]
[LTX]   latentShape: (frames: 4, height: 8, width: 12)
[LTX]   [TIME] patchifyProj: 0.000s
[LTX]   after patchifyProj: [1, 384, 4096]
[LTX]   [TIME] prepareTimestep: 0.017s
[LTX]   [TIME] prepareContext: 0.010s
[LTX]   [DIAG] RoPE cache hit (1_4_8_12)
[LTX]   [TIME] preparePositionalEmbeddings (RoPE): 0.000s
[LTX]   RoPE cos shape: [1, 32, 384, 64]
[LTX]   RoPE sin shape: [1, 32, 384, 64]
[LTX]   [TIME] all transformer blocks: 2.444s
[LTX]   [TIME] processOutput: 0.001s
[LTX]   [TIME] TOTAL transformer forward: 2.472s
[LTX]   Step 1: σ=0.9937→0.9875, vel mean=0.0262, std=1.1340, latent mean=0.0045, std=0.9903
[LTX]   Step 1 time: 2.47s
  Step 3/8 (σ=0.9875)
[LTX] Step 2: patchified [1, 384, 128], σ=0.9875
[LTX] Transformer input shapes:
[LTX]   latent: [1, 384, 128]
[LTX]   context: [1, 1024, 3840]
[LTX]   timesteps: [1]
[LTX]   latentShape: (frames: 4, height: 8, width: 12)
[LTX]   [TIME] patchifyProj: 0.000s
[LTX]   after patchifyProj: [1, 384, 4096]
[LTX]   [TIME] prepareTimestep: 0.016s
[LTX]   [TIME] prepareContext: 0.011s
[LTX]   [DIAG] RoPE cache hit (1_4_8_12)
[LTX]   [TIME] preparePositionalEmbeddings (RoPE): 0.000s
[LTX]   RoPE cos shape: [1, 32, 384, 64]
[LTX]   RoPE sin shape: [1, 32, 384, 64]
[LTX]   [TIME] all transformer blocks: 2.331s
[LTX]   [TIME] processOutput: 0.001s
[LTX]   [TIME] TOTAL transformer forward: 2.359s
[LTX]   Step 2: σ=0.9875→0.9812, vel mean=0.0274, std=1.2044, latent mean=0.0043, std=0.9872
[LTX]   Step 2 time: 2.36s
  Step 4/8 (σ=0.9812)
[LTX] Step 3: patchified [1, 384, 128], σ=0.9812
[LTX] Transformer input shapes:
[LTX]   latent: [1, 384, 128]
[LTX]   context: [1, 1024, 3840]
[LTX]   timesteps: [1]
[LTX]   latentShape: (frames: 4, height: 8, width: 12)
[LTX]   [TIME] patchifyProj: 0.000s
[LTX]   after patchifyProj: [1, 384, 4096]
[LTX]   [TIME] prepareTimestep: 0.017s
[LTX]   [TIME] prepareContext: 0.011s
[LTX]   [DIAG] RoPE cache hit (1_4_8_12)
[LTX]   [TIME] preparePositionalEmbeddings (RoPE): 0.000s
[LTX]   RoPE cos shape: [1, 32, 384, 64]
[LTX]   RoPE sin shape: [1, 32, 384, 64]
[LTX]   [TIME] all transformer blocks: 2.399s
[LTX]   [TIME] processOutput: 0.001s
[LTX]   [TIME] TOTAL transformer forward: 2.429s
[LTX]   Step 3: σ=0.9812→0.9750, vel mean=0.0285, std=1.2326, latent mean=0.0042, std=0.9839
[LTX]   Step 3 time: 2.43s
  Step 5/8 (σ=0.9750)
[LTX] Step 4: patchified [1, 384, 128], σ=0.9750
[LTX] Transformer input shapes:
[LTX]   latent: [1, 384, 128]
[LTX]   context: [1, 1024, 3840]
[LTX]   timesteps: [1]
[LTX]   latentShape: (frames: 4, height: 8, width: 12)
[LTX]   [TIME] patchifyProj: 0.001s
[LTX]   after patchifyProj: [1, 384, 4096]
[LTX]   [TIME] prepareTimestep: 0.017s
[LTX]   [TIME] prepareContext: 0.011s
[LTX]   [DIAG] RoPE cache hit (1_4_8_12)
[LTX]   [TIME] preparePositionalEmbeddings (RoPE): 0.000s
[LTX]   RoPE cos shape: [1, 32, 384, 64]
[LTX]   RoPE sin shape: [1, 32, 384, 64]
[LTX]   [TIME] all transformer blocks: 2.383s
[LTX]   [TIME] processOutput: 0.001s
[LTX]   [TIME] TOTAL transformer forward: 2.413s
[LTX]   Step 4: σ=0.9750→0.9094, vel mean=0.0271, std=1.2496, latent mean=0.0024, std=0.9534
[LTX]   Step 4 time: 2.41s
  Step 6/8 (σ=0.9094)
[LTX] Step 5: patchified [1, 384, 128], σ=0.9094
[LTX] Transformer input shapes:
[LTX]   latent: [1, 384, 128]
[LTX]   context: [1, 1024, 3840]
[LTX]   timesteps: [1]
[LTX]   latentShape: (frames: 4, height: 8, width: 12)
[LTX]   [TIME] patchifyProj: 0.005s
[LTX]   after patchifyProj: [1, 384, 4096]
[LTX]   [TIME] prepareTimestep: 0.019s
[LTX]   [TIME] prepareContext: 0.012s
[LTX]   [DIAG] RoPE cache hit (1_4_8_12)
[LTX]   [TIME] preparePositionalEmbeddings (RoPE): 0.000s
[LTX]   RoPE cos shape: [1, 32, 384, 64]
[LTX]   RoPE sin shape: [1, 32, 384, 64]
[LTX]   [TIME] all transformer blocks: 2.348s
[LTX]   [TIME] processOutput: 0.001s
[LTX]   [TIME] TOTAL transformer forward: 2.385s
[LTX]   Step 5: σ=0.9094→0.7250, vel mean=0.0088, std=1.4337, latent mean=0.0008, std=0.9057
[LTX]   Step 5 time: 2.39s
  Step 7/8 (σ=0.7250)
[LTX] Step 6: patchified [1, 384, 128], σ=0.7250
[LTX] Transformer input shapes:
[LTX]   latent: [1, 384, 128]
[LTX]   context: [1, 1024, 3840]
[LTX]   timesteps: [1]
[LTX]   latentShape: (frames: 4, height: 8, width: 12)
[LTX]   [TIME] patchifyProj: 0.000s
[LTX]   after patchifyProj: [1, 384, 4096]
[LTX]   [TIME] prepareTimestep: 0.022s
[LTX]   [TIME] prepareContext: 0.011s
[LTX]   [DIAG] RoPE cache hit (1_4_8_12)
[LTX]   [TIME] preparePositionalEmbeddings (RoPE): 0.000s
[LTX]   RoPE cos shape: [1, 32, 384, 64]
[LTX]   RoPE sin shape: [1, 32, 384, 64]
[LTX]   [TIME] all transformer blocks: 2.399s
[LTX]   [TIME] processOutput: 0.001s
[LTX]   [TIME] TOTAL transformer forward: 2.434s
[LTX]   Step 6: σ=0.7250→0.4219, vel mean=0.0119, std=1.4132, latent mean=-0.0028, std=0.9915
[LTX]   Step 6 time: 2.43s
  Step 8/8 (σ=0.4219)
[LTX] Step 7: patchified [1, 384, 128], σ=0.4219
[LTX] Transformer input shapes:
[LTX]   latent: [1, 384, 128]
[LTX]   context: [1, 1024, 3840]
[LTX]   timesteps: [1]
[LTX]   latentShape: (frames: 4, height: 8, width: 12)
[LTX]   [TIME] patchifyProj: 0.000s
[LTX]   after patchifyProj: [1, 384, 4096]
[LTX]   [TIME] prepareTimestep: 0.017s
[LTX]   [TIME] prepareContext: 0.011s
[LTX]   [DIAG] RoPE cache hit (1_4_8_12)
[LTX]   [TIME] preparePositionalEmbeddings (RoPE): 0.000s
[LTX]   RoPE cos shape: [1, 32, 384, 64]
[LTX]   RoPE sin shape: [1, 32, 384, 64]
[LTX]   [TIME] all transformer blocks: 2.373s
[LTX]   [TIME] processOutput: 0.001s
[LTX]   [TIME] TOTAL transformer forward: 2.403s
[LTX]   Step 7: σ=0.4219→0.0000, vel mean=0.0178, std=1.3826, latent mean=-0.0103, std=1.3765
[LTX]   Step 7 time: 2.40s
[LTX] Stage 1 complete: 20.6s
[LTX] Stage 1 latent stats: mean=-0.010328754, std=1.3765082
[LTX] === Upscaling latent 2x ===
[LTX] Loading spatial upscaler from /Users/vincent/Library/Caches/models/ltx-upscaler/diffusion_pytorch_model.safetensors...
[LTX] Detected mid_channels: 1024
[LTX] Upscaler model has 72 parameter tensors, loaded 72 from file
[LTX] Verify initial_conv.weight: shape=[1024, 3, 3, 3, 128], mean=-0.00021665228, std=0.024886075
[LTX] Verify res_blocks.0.conv1.weight: shape=[1024, 3, 3, 3, 1024], mean=0.0008435387
[LTX] Upscaler parameter keys (72):
[LTX]   final_conv.bias: [128]
[LTX]   final_conv.weight: [128, 3, 3, 3, 1024]
[LTX]   initial_conv.bias: [1024]
[LTX]   initial_conv.weight: [1024, 3, 3, 3, 128]
[LTX]   initial_norm.bias: [1024]
[LTX]   initial_norm.weight: [1024]
[LTX]   post_upsample_res_blocks.0.conv1.bias: [1024]
[LTX]   post_upsample_res_blocks.0.conv1.weight: [1024, 3, 3, 3, 1024]
[LTX]   post_upsample_res_blocks.0.conv2.bias: [1024]
[LTX]   post_upsample_res_blocks.0.conv2.weight: [1024, 3, 3, 3, 1024]
[LTX]   ... and 62 more
[LTX] VAE stats: mean shape=[128], std shape=[128]
[LTX] After denormalize: mean=-0.00068451883, std=0.19258666
[LTX] After upscaler: shape=[1, 128, 4, 16, 24], mean=-0.0024255966, std=0.16709217
[LTX] After renormalize: mean=-0.013053498, std=1.0901163
[LTX] After AdaIN: mean=-0.010328751, std=1.3765082
[LTX] Upscaled latent shape: [1, 128, 4, 16, 24]
[LTX] Upscale time: 0.4s
[LTX] === Stage 2: Full-resolution refinement (3 steps) ===
[LTX] Stage 2 latent: 4x16x24
[LTX] Added stage 2 noise (σ=0.909375)
[LTX] Stage 2 input stats: mean=0.0008213022, std=0.92053694
  Step 1/3 (σ=0.9094)
[LTX] Step 0: patchified [1, 1536, 128], σ=0.9094
[LTX] Transformer input shapes:
[LTX]   latent: [1, 1536, 128]
[LTX]   context: [1, 1024, 3840]
[LTX]   timesteps: [1]
[LTX]   latentShape: (frames: 4, height: 16, width: 24)
[LTX]   [TIME] patchifyProj: 0.001s
[LTX]   after patchifyProj: [1, 1536, 4096]
[LTX]   [TIME] prepareTimestep: 0.020s
[LTX]   [TIME] prepareContext: 0.020s
[LTX]   [TIME] preparePositionalEmbeddings (RoPE): 0.781s
[LTX]   RoPE cos shape: [1, 32, 1536, 64]
[LTX]   RoPE sin shape: [1, 32, 1536, 64]
[LTX]   [TIME] all transformer blocks: 7.625s
[LTX]   [TIME] processOutput: 0.002s
[LTX]   [TIME] TOTAL transformer forward: 8.449s
[LTX]   Step 0: σ=0.9094→0.7250, vel mean=0.0076, std=1.4180, latent mean=-0.0006, std=0.8599
[LTX]   Step 0 time: 8.45s
  Step 2/3 (σ=0.7250)
[LTX] Step 1: patchified [1, 1536, 128], σ=0.7250
[LTX] Transformer input shapes:
[LTX]   latent: [1, 1536, 128]
[LTX]   context: [1, 1024, 3840]
[LTX]   timesteps: [1]
[LTX]   latentShape: (frames: 4, height: 16, width: 24)
[LTX]   [TIME] patchifyProj: 0.001s
[LTX]   after patchifyProj: [1, 1536, 4096]
[LTX]   [TIME] prepareTimestep: 0.015s
[LTX]   [TIME] prepareContext: 0.011s
[LTX]   [DIAG] RoPE cache hit (1_4_16_24)
[LTX]   [TIME] preparePositionalEmbeddings (RoPE): 0.000s
[LTX]   RoPE cos shape: [1, 32, 1536, 64]
[LTX]   RoPE sin shape: [1, 32, 1536, 64]
[LTX]   [TIME] all transformer blocks: 7.623s
[LTX]   [TIME] processOutput: 0.001s
[LTX]   [TIME] TOTAL transformer forward: 7.651s
[LTX]   Step 1: σ=0.7250→0.4219, vel mean=0.0062, std=1.4658, latent mean=-0.0025, std=0.9504
[LTX]   Step 1 time: 7.65s
  Step 3/3 (σ=0.4219)
[LTX] Step 2: patchified [1, 1536, 128], σ=0.4219
[LTX] Transformer input shapes:
[LTX]   latent: [1, 1536, 128]
[LTX]   context: [1, 1024, 3840]
[LTX]   timesteps: [1]
[LTX]   latentShape: (frames: 4, height: 16, width: 24)
[LTX]   [TIME] patchifyProj: 0.001s
[LTX]   after patchifyProj: [1, 1536, 4096]
[LTX]   [TIME] prepareTimestep: 0.018s
[LTX]   [TIME] prepareContext: 0.011s
[LTX]   [DIAG] RoPE cache hit (1_4_16_24)
[LTX]   [TIME] preparePositionalEmbeddings (RoPE): 0.000s
[LTX]   RoPE cos shape: [1, 32, 1536, 64]
[LTX]   RoPE sin shape: [1, 32, 1536, 64]
[LTX]   [TIME] all transformer blocks: 7.669s
[LTX]   [TIME] processOutput: 0.001s
[LTX]   [TIME] TOTAL transformer forward: 7.700s
[LTX]   Step 2: σ=0.4219→0.0000, vel mean=0.0073, std=1.4568, latent mean=-0.0056, std=1.3592
[LTX]   Step 2 time: 7.70s
[LTX] Stage 2 complete: 23.8s
[LTX] Stage 2 output stats: mean=-0.005550382, std=1.359192, min=-7.3749104, max=7.2209044
[LTX] Transformer unloaded for VAE decode phase
[LTX] [MEM] after transformer unload: active=28862MB peak=52943MB cache=0MB
[LTX] [MEM] Phase vaeDecode: cache limit set to 512MB
[LTX] Decoding latents to video... (timestep=nil)
[LTX] VAE Decoder input: [1, 128, 4, 16, 24]
[LTX] After denormalize: mean=-0.0010638044
[LTX] After conv_in: [1, 1024, 4, 16, 24], mean=0.03905001
[LTX] After up_blocks_0 (res): [1, 1024, 4, 16, 24], mean=0.005725174
[LTX] After up_blocks_1 (d2s): [1, 512, 7, 32, 48], mean=-0.11551167
[LTX] After up_blocks_2 (res): [1, 512, 7, 32, 48], mean=-0.008380226
[LTX] After up_blocks_3 (d2s): [1, 256, 13, 64, 96], mean=-0.26394233
[LTX] After up_blocks_4 (res): [1, 256, 13, 64, 96], mean=-0.042972304
[LTX] After up_blocks_5 (d2s): [1, 128, 25, 128, 192], mean=-0.69668806
[LTX] After up_blocks_6 (res): [1, 128, 25, 128, 192], mean=-2.4112833
[LTX] After conv_out: [1, 48, 25, 128, 192], mean=-0.26804453
[LTX] After unpatchify: [1, 3, 25, 512, 768]
[LTX] VAE raw output: mean=-0.26804456, min=-1.501929, max=1.7457318
[LTX] VAE decode: 4.1s
[LTX] [MEM] after two-stage VAE decode: active=29982MB peak=52943MB cache=412MB
[LTX] Total two-stage time: 81.2s
Generation completed in 81.4s

Exporting to docs/examples/beaver-dam/distilled-upscaler.mp4...
Video saved to: /Users/vincent/Developpements/ltx-video-swift-mlx/docs/examples/beaver-dam/distilled-upscaler.mp4

--- Summary ---
Frames: 25
Resolution: 768x512
Seed: 42
Generation time: 81.2s

--- Profiling ---
Text Encoding (Gemma + FE + Connector): 5.2s
Denoising (11 steps):                 44.4s
  Step 0: 3.6s
  Step 1: 2.5s
  Step 2: 2.4s
  Step 3: 2.4s
  Step 4: 2.4s
  Step 5: 2.4s
  Step 6: 2.4s
  Step 7: 2.4s
  Step 8: 8.4s
  Step 9: 7.7s
  Step 10: 7.7s
  Average per step:                      4.0s
VAE Decoding:                            4.1s
Model Loading:                           15.7s
Pipeline total (excl. loading/export):   53.6s

--- Memory ---
Peak GPU memory:                         52943 MB
Mean GPU memory (denoising):              27467 MB
